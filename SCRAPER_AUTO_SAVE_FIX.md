# Scraper Auto-Save Fix - 0 Results Issue Resolved âœ…

## Problem

Sessions were showing "0 results" in the import list because:
1. When scraping starts, a session is created in the database with `status: 'running'`
2. Businesses are NOT saved to the database during scraping
3. Businesses are only saved when you manually click "Save" button
4. If you don't click "Save", the session exists but has no businesses
5. The import list shows "0 results" because no businesses are in the database

## Root Cause

The scraper was designed to require manual saving after scraping completes. This meant:
- Session exists in database immediately
- Businesses only exist in memory during scraping
- Businesses only saved to database when user clicks "Save"
- If user forgets to save, session appears empty

## Solution

Added **automatic session saving** when scraping completes:

### What Was Changed

**File: `app/api/scraper/start/route.ts`**

Added an event listener for the `'complete'` event that automatically saves the session and businesses to the database when scraping finishes:

```typescript
// Listen for completion event to auto-save session
eventEmitter.once('complete', async () => {
  try {
    console.log(`[SCRAPER API] Session ${sessionId} completed, auto-saving...`);
    
    const loggingManager = orchestrator.getLoggingManager();
    const businesses = orchestrator.getResults();
    const summary = loggingManager.getSummary();
    const progress = orchestrator.getProgress();

    const client = await pool.connect();
    try {
      await client.query('BEGIN');

      // Update session with summary
      await client.query(
        `UPDATE scraping_sessions 
         SET summary = $1, status = $2, progress = $3, updated_at = NOW()
         WHERE id = $4`,
        [
          JSON.stringify({
            totalBusinesses: businesses.length,
            townsCompleted: progress.completedTowns,
            errors: summary.totalErrors,
            totalDuration: summary.totalDuration,
            averageDuration: summary.averageDuration,
          }),
          'completed',
          100,
          sessionId,
        ]
      );

      // Save businesses to database using batch operations
      if (businesses.length > 0) {
        const { batchInsertBusinesses } = await import('@/lib/scraper/batchOperations');
        await batchInsertBusinesses(client, sessionId, businesses);
      }

      await client.query('COMMIT');
      console.log(`[SCRAPER API] Session ${sessionId} auto-saved successfully with ${businesses.length} businesses`);
    } catch (error) {
      await client.query('ROLLBACK');
      console.error(`[SCRAPER API] Error auto-saving session ${sessionId}:`, error);
    } finally {
      client.release();
    }
  } catch (error) {
    console.error(`[SCRAPER API] Error in completion handler for session ${sessionId}:`, error);
  }
});
```

## How It Works Now

### Scraping Flow:

1. **Start Scraping**
   - Session created in database with meaningful name
   - Status: `'running'`
   - Businesses: 0 (not saved yet)

2. **During Scraping**
   - Businesses collected in memory
   - Progress updates sent to UI
   - Session status remains `'running'`

3. **Scraping Completes** âœ¨ **NEW AUTO-SAVE**
   - Orchestrator emits `'complete'` event
   - Event listener automatically:
     - Updates session status to `'completed'`
     - Updates session summary with business count
     - Saves all businesses to database
     - Sets progress to 100%

4. **Import from Scraper**
   - Session now shows correct business count
   - Session is clickable (status: 'completed')
   - Businesses are available for preview and import

### Optional Manual Save:

The "Save" button still works and allows you to:
- Rename the session with a custom name
- Manually trigger save if auto-save failed
- Re-save with updated name

## Testing

### Test the Fix:

1. **Start a new scrape:**
   - Go to Scraper page
   - Enter towns and select industries
   - Click "Start Scraping"

2. **Wait for completion:**
   - Watch the progress bar
   - Wait for status to show "Completed"
   - **DO NOT click "Save" button**

3. **Check import list:**
   - Go to Leads â†’ Import from Scraper
   - You should see your session with correct business count
   - Session should be clickable
   - Preview should show businesses

### Expected Results:

âœ… Session shows correct business count (not "0 results")
âœ… Session is clickable when completed
âœ… Preview shows actual businesses
âœ… Import works correctly
âœ… No manual "Save" required

## Benefits

1. **No More "0 Results"** - Sessions always have businesses after completion
2. **Automatic** - No need to remember to click "Save"
3. **Immediate Availability** - Sessions appear in import list right away
4. **Better UX** - Users don't need to understand the save process
5. **Backwards Compatible** - Manual "Save" button still works for renaming

## Migration Notes

### Existing Sessions:

Old sessions that were never saved will still show "0 results" because they don't have businesses in the database. These sessions were created before the auto-save feature.

**To fix old sessions:**
- They cannot be recovered (businesses were only in memory)
- Just run new scrapes - they will auto-save correctly

### New Sessions:

All new scrapes will automatically save when they complete. No action required from users.

## Result

The scraper now works seamlessly:
- âœ… Start scraping â†’ Session created with meaningful name
- âœ… Scraping completes â†’ Session and businesses auto-saved
- âœ… Go to import â†’ Session shows correct business count
- âœ… Click session â†’ Preview and import work perfectly

No more "0 results" issue! ðŸŽ‰
